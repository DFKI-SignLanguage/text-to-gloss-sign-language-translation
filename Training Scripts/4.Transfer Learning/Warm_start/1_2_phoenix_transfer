#!/bin/bash
pip install sacrebleu
echo "export LC_ALL=C">>~/.bashrc
source ~/.bashrc

OLDMODELNAME=phoenix_baseline_transformer_lemmnorm_pretrain_bpe_32k_dede
MODELNAME=phoenix_baseline_transformer_lemmnorm_pretrain_bpe_32k_dede_transfered2
PWD=/netscratch/zhu/submission
SEC=$PWD/section4
TRAINTEXT=$PWD/data/t2g_phoenix/train/phoenix_sentences_train_lower_lemm_norm.txt
TRAINGLOSS=$PWD/data/t2g_phoenix/train/phoenix_glosses_train_lower.txt
DEVTEXT=$PWD/data/t2g_phoenix/dev/phoenix_sentences_dev_lower_lemm_norm.txt
DEVGLOSS=$PWD/data/t2g_phoenix/dev/phoenix_glosses_dev_lower.txt
TESTTEXT=$PWD/data/t2g_phoenix/test/phoenix_sentences_test_lower_lemm_norm.txt
TESTGLOSS=$PWD/data/t2g_phoenix/test/phoenix_glosses_test_lower.txt
mosesdecoder=$PWD/mosesdecoder
subword_nmt=$PWD/subword-nmt/subword_nmt
SRC=de
TRG=en
bpe_operations=32000
mkdir -p $SEC/model/$MODELNAME
mkdir -p $SEC/model/$MODELNAME/output
path=$SEC/model/$MODELNAME

transfer_data=$SEC/model/data_transfer_learningnew
cp $SEC/model/$OLDMODELNAME/vocab.deen.yml $path/vocab.deen.yml
        /marian/build/marian \
        --devices 0 1 \
        --model $path/$MODELNAME.npz \
        --type transformer \
	--no-restore-corpus \
        --pretrained-model $SEC/model/$OLDMODELNAME/$OLDMODELNAME.npz.best-bleu-detok.npz \
        --layer-normalization \
        --maxi-batch 32 --valid-reset-stalled \
        --quiet-translation \
        --learn-rate 0.0003 --lr-warmup 16000 \
        --lr-decay-inv-sqrt 16000 --lr-report \
        --optimizer adam \
        --early-stopping 10 \
        --cost-type=ce-mean-words \
        --valid-metrics perplexity bleu-detok ce-mean-words translation chrf bleu \
        --train-sets $transfer_data/train.bpe.de $transfer_data/train.bpe.en \
        --vocabs $path/vocab.deen.yml $path/vocab.deen.yml \
        --valid-sets $transfer_data/dev.bpe.de $transfer_data/dev.bpe.en \
        --log $path/train.log \
        --valid-log $path/valid.log --valid-mini-batch 32 \
        --valid-script-path $SEC/valid.sh \
        --valid-translation-output $path/output/dev_output_pre.txt \
        --keep-best \
        --valid-freq 100 \
        --disp-freq 1000 \
        --dim-emb 512 --transformer-dim-ffn 2048 --transformer-heads 8 --transformer-ffn-activation relu --enc-depth 6 --dec-depth 6 --enc-cell lstm \
        --enc-cell-depth 2 --dec-cell-base-depth 2 --dec-cell lstm \
        --sync-sgd --dropout-src 0.1 --dropout-trg 0.1 \
        --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --beam-size 6 --normalize 0.6 \
        --exponential-smoothing --seed 1111 \
        --tied-embeddings-all \
        --transformer-dropout 0.2 --label-smoothing 0.1 \

#:<<!
cat $transfer_data/dev.bpe.de | /marian/build/marian-decoder -c $path/$MODELNAME.npz.best-bleu-detok.npz.decoder.yml -d 0 1 -b 6 -n 0.6 \
        --mini-batch 64 --maxi-batch 100 --maxi-batch-sort src \
        | sed 's/\@\@ //g' \
        | $mosesdecoder/scripts/tokenizer/detokenizer.perl -l de \
        > $path/output/dev_output.txt

cat $transfer_data/test.bpe.de | /marian/build/marian-decoder -c $path/$MODELNAME.npz.best-bleu-detok.npz.decoder.yml -d 0 1 -b 6 -n 0.6 \
        --mini-batch 64 --maxi-batch 100 --maxi-batch-sort src \
        | sed 's/\@\@ //g' \
        | $mosesdecoder/scripts/tokenizer/detokenizer.perl -l de \
        > $path/output/test_output.txt
echo "------------------BLEU SCORE OF DEV--------------------"
$PWD/sacrebleu/sacrebleu/sacrebleu.py $DEVGLOSS -i $SEC/model/$MODELNAME/output/dev_output.txt -m bleu chrf ter -b -w 4 -tok none
echo "------------------BLEU SCORE OF TEST-------------------"
$PWD/sacrebleu/sacrebleu/sacrebleu.py $TESTGLOSS -i $SEC/model/$MODELNAME/output/test_output.txt -m bleu chrf ter -b -w 4 -tok none


