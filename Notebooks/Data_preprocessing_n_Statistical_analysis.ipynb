{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b97lS948T5E"
      },
      "source": [
        "Stemming & Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCbvgOao9kYL",
        "outputId": "020aff52-ff04-4ad5-dd16-98e9445b4fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 102 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 122 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 133 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 153 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 163 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 174 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 184 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 194 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 204 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 215 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 225 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 235 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 245 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 256 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 266 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 276 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 286 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 296 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 307 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 317 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 327 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 337 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 348 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 358 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 368 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 378 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 389 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 399 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 409 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 419 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 430 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 440 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 450 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 460 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 471 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 481 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 491 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 501 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 512 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 522 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 532 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 542 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 552 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 563 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 573 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 583 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 593 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 604 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 614 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 624 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 634 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 645 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 655 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 665 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 675 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 686 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 696 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 706 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 716 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 727 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 737 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 747 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 757 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 768 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 778 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 788 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 798 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 808 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 819 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 829 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 839 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 849 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 860 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 870 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 880 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 890 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 901 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 911 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 921 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 931 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 942 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 952 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 962 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 972 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 983 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 993 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.0 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.0 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.0 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.0 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 4.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install HanTa -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYL3q18n57pV"
      },
      "outputs": [],
      "source": [
        "!mkdir -p train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlpIBR-v_W7O",
        "outputId": "41a74e4b-d419-45f7-9bf3-8067ee924d99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def stemming(filename):\n",
        "  file = open(filename,'r')\n",
        "  txt = file.readlines()\n",
        "  a = []\n",
        "  for w in txt:\n",
        "    w = w.replace('\\n','')\n",
        "    a.append(w)\n",
        "\n",
        "  ps = SnowballStemmer('german')\n",
        "  data_list_s = [] \n",
        "  for words in a:\n",
        "    words = word_tokenize(words)\n",
        "    words_s = ''\n",
        "    for w in words:\n",
        "        w_s = ps.stem(w)\n",
        "        words_s += w_s + ' '\n",
        "    data_list_s.append(words_s)\n",
        "  with open('./train/{}_stem.txt'.format(filename.strip(\".txt\")),'w') as f:\n",
        "     for i in data_list_s:\n",
        "       k = i.lower()\n",
        "       f.write(k+'\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbbSvlR9-Urs"
      },
      "outputs": [],
      "source": [
        "from HanTa import HanoverTagger as ht\n",
        "tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "\n",
        "def lemmatization(filename):\n",
        "  file = open(filename,'r')\n",
        "  txt = file.readlines()\n",
        "  a = []\n",
        "  for w in txt:\n",
        "    w = w.replace('\\n','')\n",
        "    a.append(w)\n",
        "  lemmas = []\n",
        "  for mail in a:\n",
        "    lemma = [lemma for (word,lemma,pos) in tagger.tag_sent(mail.split())]\n",
        "    lemmas.append(' '.join(lemma))\n",
        "\n",
        "  with open('./train/{}_lemm.txt'.format(filename.strip(\".txt\")),'w') as f:\n",
        "     for i in lemmas:\n",
        "       k = i.lower()\n",
        "       f.write(k+'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT3jYLOIC8Sq"
      },
      "outputs": [],
      "source": [
        "stemming('phoenix_sentences_dev_lower.txt')\n",
        "lemmatization('phoenix_sentences_dev_lower.txt')\n",
        "stemming('phoenix_sentences_train_lower.txt')\n",
        "lemmatization('phoenix_sentences_train_lower.txt')\n",
        "stemming('phoenix_sentences_test_lower.txt')\n",
        "lemmatization('phoenix_sentences_test_lower.txt')\n",
        "\n",
        "stemming('dgs_sentences_dev_lower.txt')\n",
        "lemmatization('dgs_sentences_dev_lower.txt')\n",
        "stemming('dgs_sentences_train_lower.txt')\n",
        "lemmatization('dgs_sentences_train_lower.txt')\n",
        "stemming('dgs_sentences_test_lower.txt')\n",
        "lemmatization('dgs_sentences_test_lower.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBIVN1KIT2Xw"
      },
      "source": [
        "DGS stripping algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPJfbZ3DPvPP",
        "outputId": "b7efe833-fdfd-4629-81c6-dd71581c4be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'easier-gloss-translation'...\n",
            "remote: Enumerating objects: 1318, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 1318 (delta 36), reused 39 (delta 17), pack-reused 1252\u001b[K\n",
            "Receiving objects: 100% (1318/1318), 158.15 KiB | 12.17 MiB/s, done.\n",
            "Resolving deltas: 100% (926/926), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bricksdont/easier-gloss-translation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpxM_urgPx4J",
        "outputId": "a8e68803-1c1b-4dc2-d3ef-419ece3d4548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG:root:Namespace(generalize_dgs_glosses='false', input_file='./dgs_glosses_train_upper_all_cleaned.txt', lang='dgs_de', lowercase_glosses='true', output_file='./dgs_glosses_train_lower_all_cleaned.txt', use_mouthing_tier=False)\n"
          ]
        }
      ],
      "source": [
        "!python3 ./easier-gloss-translation/scripts/preprocessing/preprocess_glosses.py --input-file ./dgs_glosses_train_upper_all_cleaned.txt --output-file ./dgs_glosses_train_lower_all_cleaned.txt --lang dgs_de --lowercase-glosses true --generalize-dgs-glosses false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8-kKe3bT7lA"
      },
      "source": [
        "Words counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPgvlGrP8EL7"
      },
      "outputs": [],
      "source": [
        "#-*- coding: UTF-8 -*-\n",
        "\n",
        "def get_words(filename):\n",
        "    with open(filename) as f:\n",
        "        content = f.read()\n",
        "\n",
        "        #content = content.replace(\"-\", \" \")\n",
        "        #content = content.replace(\".\", \"\")\n",
        "        #content = content.replace(\"!\", \"\")\n",
        "        #content = content.replace(\"?\", \"\")\n",
        "\n",
        "        content = content.strip()\n",
        "\n",
        "        words = [word for word in content.split()]\n",
        "        # print(words)\n",
        "        return words\n",
        "\n",
        "def count_words(filename):\n",
        "\n",
        "    words = get_words(filename)\n",
        "\n",
        "    words_amount = dict.fromkeys(words, 0)\n",
        "\n",
        "    for word in words_amount.keys():\n",
        "\n",
        "        cnt = words.count(word)\n",
        "        words_amount[word] = cnt\n",
        "\n",
        "    words_amount = sorted(\n",
        "        words_amount.items(), \n",
        "        key=lambda x:x[1], \n",
        "        reverse=True\n",
        "    )\n",
        "    return words_amount\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-i3pR77WL3Y"
      },
      "source": [
        "Phoenix_stastics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7RQ_PAf_Pbv"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./phoenix_sentences_train_lower.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./phoenix_sentences_train_lower.txt\")\n",
        "p_words_train_glo= get_words(\"./phoenix_glosses_train_lower.txt\")\n",
        "p_words_amount_train_glo = count_words(\"./phoenix_glosses_train_lower.txt\")\n",
        "\n",
        "p_words_dev_sen = get_words(\"./phoenix_sentences_dev_lower.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./phoenix_sentences_dev_lower.txt\")\n",
        "p_words_dev_glo= get_words(\"./phoenix_glosses_dev_lower.txt\")\n",
        "p_words_amount_dev_glo = count_words(\"./phoenix_glosses_dev_lower.txt\")\n",
        "\n",
        "p_words_test_sen = get_words(\"./phoenix_sentences_test_lower.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./phoenix_sentences_test_lower.txt\")\n",
        "p_words_test_glo= get_words(\"./phoenix_glosses_test_lower.txt\")\n",
        "p_words_amount_test_glo = count_words(\"./phoenix_glosses_test_lower.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxx9__PUWy7G",
        "outputId": "50975c60-492e-4c6b-8d2a-82cf7d09a624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FOR TRAIN SENTENCES\n",
            "vocab 2887\n",
            "tital words 99081\n",
            "singleton 1077\n",
            "FOR TRAIN GLOSSES\n",
            "vocab 1085\n",
            "tital words 55247\n",
            "singleton 355\n",
            "FOR DEV SENTENCES\n",
            "vocab 951\n",
            "tital words 6820\n",
            "oov 57\n",
            "FOR DEV GLOSSES\n",
            "vocab 393\n",
            "tital words 3748\n",
            "oov 14\n",
            "FOR TEST SENTENCES\n",
            "vocab 1001\n",
            "tital words 7816\n",
            "oov 60\n",
            "FOR TEST GLOSSES\n",
            "vocab 411\n",
            "tital words 4264\n",
            "oov 19\n"
          ]
        }
      ],
      "source": [
        "print('PHOENIX STATISTICS ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "#train glosses\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_glo:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_glo:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#dev glosses \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_glo:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_glo:\n",
        "  if i not in p_words_train_glo:\n",
        "    b+=1\n",
        "print('FOR DEV GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_dev_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test glosses \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_glo:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_glo:\n",
        "  if i not in p_words_train_glo:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mt4ywx9XAaC"
      },
      "source": [
        "Phoenix_stem_statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq34ptIyW_H1"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./train/phoenix_sentences_train_lower_stem.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./train/phoenix_sentences_train_lower_stem.txt\")\n",
        "\n",
        "p_words_dev_sen = get_words(\"./train/phoenix_sentences_dev_lower_stem.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./train/phoenix_sentences_dev_lower_stem.txt\")\n",
        "\n",
        "p_words_test_sen = get_words(\"./train/phoenix_sentences_test_lower_stem.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./train/phoenix_sentences_test_lower_stem.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTzf-sUTXdO0",
        "outputId": "17e4a791-756a-4e89-a8ff-130cf5409266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PHOENIX STATISTICS STEM \n",
            "FOR TRAIN SENTENCES\n",
            "vocab 2154\n",
            "tital words 99081\n",
            "singleton 764\n",
            "FOR DEV SENTENCES\n",
            "vocab 765\n",
            "tital words 6820\n",
            "oov 42\n",
            "FOR TEST SENTENCES\n",
            "vocab 797\n",
            "tital words 7816\n",
            "oov 39\n"
          ]
        }
      ],
      "source": [
        "print('PHOENIX STATISTICS STEM ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xfLfhaGYEFP"
      },
      "source": [
        "Phoenix_statics_lemm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSe5WKL1YG9k"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./train/phoenix_sentences_train_lower_lemm_norm.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./train/phoenix_sentences_train_lower_lemm_norm.txt\")\n",
        "\n",
        "p_words_dev_sen = get_words(\"./train/phoenix_sentences_dev_lower_lemm_norm.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./train/phoenix_sentences_dev_lower_lemm_norm.txt\")\n",
        "\n",
        "p_words_test_sen = get_words(\"./train/phoenix_sentences_test_lower_lemm_norm.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./train/phoenix_sentences_test_lower_lemm_norm.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfeG7TLHYG59",
        "outputId": "f562ca90-e04e-4260-9b0c-f3fa84b7b8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PHOENIX STATISTICS LEMM \n",
            "FOR TRAIN SENTENCES\n",
            "vocab 2216\n",
            "tital words 99081\n",
            "singleton 765\n",
            "FOR DEV SENTENCES\n",
            "vocab 793\n",
            "tital words 6820\n",
            "oov 39\n",
            "FOR TEST SENTENCES\n",
            "vocab 836\n",
            "tital words 7816\n",
            "oov 38\n"
          ]
        }
      ],
      "source": [
        "print('PHOENIX STATISTICS LEMM ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzSKtVR1YaRV"
      },
      "source": [
        "DGS_STATIC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8qax5BAYZh9"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./dgs_sentences_train_lower.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./dgs_sentences_train_lower.txt\")\n",
        "p_words_train_glo= get_words(\"./dgs_glosses_train_lower.txt\")\n",
        "p_words_amount_train_glo = count_words(\"./dgs_glosses_train_lower.txt\")\n",
        "\n",
        "p_words_dev_sen = get_words(\"./dgs_sentences_dev_lower.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./dgs_sentences_dev_lower.txt\")\n",
        "p_words_dev_glo= get_words(\"./dgs_glosses_dev_lower.txt\")\n",
        "p_words_amount_dev_glo = count_words(\"./dgs_glosses_dev_lower.txt\")\n",
        "\n",
        "p_words_test_sen = get_words(\"./dgs_sentences_test_lower.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./dgs_sentences_test_lower.txt\")\n",
        "p_words_test_glo= get_words(\"./dgs_glosses_test_lower.txt\")\n",
        "p_words_amount_test_glo = count_words(\"./dgs_glosses_test_lower.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddjpgicQYZfU",
        "outputId": "08e35e98-c7cd-463a-e712-14934aa773d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGS STATISTICS \n",
            "FOR TRAIN SENTENCES\n",
            "vocab 20868\n",
            "tital words 472609\n",
            "singleton 9946\n",
            "FOR TRAIN GLOSSES\n",
            "vocab 19521\n",
            "tital words 301772\n",
            "singleton 6286\n",
            "FOR DEV SENTENCES\n",
            "vocab 4617\n",
            "tital words 36629\n",
            "oov 971\n",
            "FOR DEV GLOSSES\n",
            "vocab 4894\n",
            "tital words 21715\n",
            "oov 614\n",
            "FOR TEST SENTENCES\n",
            "vocab 4992\n",
            "tital words 44452\n",
            "oov 1080\n",
            "FOR TEST GLOSSES\n",
            "vocab 5688\n",
            "tital words 28405\n",
            "oov 752\n"
          ]
        }
      ],
      "source": [
        "print('DGS STATISTICS ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "#train glosses\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_glo:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_glo:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#dev glosses \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_glo:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_glo:\n",
        "  if i not in p_words_train_glo:\n",
        "    b+=1\n",
        "print('FOR DEV GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_dev_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test glosses \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_glo:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_glo:\n",
        "  if i not in p_words_train_glo:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST GLOSSES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_glo)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN0IdipVZIQY"
      },
      "source": [
        "DGS STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZJrUcuaYZcp"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./train/dgs_sentences_train_lower_stem.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./train/dgs_sentences_train_lower_stem.txt\")\n",
        " \n",
        "\n",
        "p_words_dev_sen = get_words(\"./train/dgs_sentences_dev_lower_stem.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./train/dgs_sentences_dev_lower_stem.txt\")\n",
        " \n",
        "\n",
        "p_words_test_sen = get_words(\"./train/dgs_sentences_test_lower_stem.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./train/dgs_sentences_test_lower_stem.txt\")\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miZpaaqYYZZ2",
        "outputId": "90fe080c-cc84-4d0d-bf04-04a1a6d4b9f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGS STATISTICS STEM \n",
            "FOR TRAIN SENTENCES\n",
            "vocab 15504\n",
            "tital words 472751\n",
            "singleton 6995\n",
            "FOR DEV SENTENCES\n",
            "vocab 3618\n",
            "tital words 36645\n",
            "oov 709\n",
            "FOR TEST SENTENCES\n",
            "vocab 3925\n",
            "tital words 44477\n",
            "oov 751\n"
          ]
        }
      ],
      "source": [
        "print('DGS STATISTICS STEM ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgBUP3fOZhRr"
      },
      "source": [
        "DGS LEMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE8VrNoFYZXm"
      },
      "outputs": [],
      "source": [
        "p_words_train_sen = get_words(\"./train/dgs_sentences_train_lower_lemm.txt\")\n",
        "p_words_amount_train_sen = count_words(\"./train/dgs_sentences_train_lower_lemm.txt\")\n",
        " \n",
        "\n",
        "p_words_dev_sen = get_words(\"./train/dgs_sentences_dev_lower_lemm.txt\")\n",
        "p_words_amount_dev_sen = count_words(\"./train/dgs_sentences_dev_lower_lemm.txt\")\n",
        " \n",
        "\n",
        "p_words_test_sen = get_words(\"./train/dgs_sentences_test_lower_lemm.txt\")\n",
        "p_words_amount_test_sen = count_words(\"./train/dgs_sentences_test_lower_lemm.txt\")\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rku-4mnDYZQO",
        "outputId": "04ba197a-0a63-40ad-f0d3-13a757f0b864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGS STATISTICS LEMM \n",
            "FOR TRAIN SENTENCES\n",
            "vocab 15170\n",
            "tital words 472609\n",
            "singleton 6929\n",
            "FOR DEV SENTENCES\n",
            "vocab 3497\n",
            "tital words 36629\n",
            "oov 691\n",
            "FOR TEST SENTENCES\n",
            "vocab 3791\n",
            "tital words 44452\n",
            "oov 773\n"
          ]
        }
      ],
      "source": [
        "print('DGS STATISTICS LEMM ')\n",
        "#train sentences\n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_train_sen:\n",
        "  a+= i[1]\n",
        "\n",
        "for i in p_words_amount_train_sen:\n",
        "  if i[1]==1:\n",
        "    b+=1\n",
        "print('FOR TRAIN SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_train_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('singleton {}'.format(b))\n",
        "\n",
        "#dev sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_dev_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_dev_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "print('FOR DEV SENTENCES')\n",
        "\n",
        "print('vocab {}'.format(len(p_words_amount_dev_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n",
        "#test sentence \n",
        "a=0\n",
        "b=0\n",
        "for i in p_words_amount_test_sen:\n",
        "  a+= i[1]\n",
        "for i in p_words_test_sen:\n",
        "  if i not in p_words_train_sen:\n",
        "    b+=1\n",
        "\n",
        "print('FOR TEST SENTENCES')\n",
        "print('vocab {}'.format(len(p_words_amount_test_sen)))\n",
        "print('tital words {}'.format(a))\n",
        "print('oov {}'.format(b))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
